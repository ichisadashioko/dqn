{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionTable:\n",
    "    def __init__(\n",
    "        self,\n",
    "        stateDim=(105, 80),\n",
    "        histLen=1,\n",
    "        maxSize=1_000_000,\n",
    "        bufferSize=1024,\n",
    "    ):\n",
    "        self.stateDim = stateDim\n",
    "        self.histLen = histLen\n",
    "        self.maxSize = maxSize\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buf_ind = None\n",
    "\n",
    "        self.recentMemSize = self.histLen\n",
    "\n",
    "        self.numEntries = 0\n",
    "        self.insertIndex = 0\n",
    "\n",
    "        # The original implementation has multiple `histType`, we are going to use 'linear' only. Because of that, there is no `histIndices`\n",
    "\n",
    "        # DONE pre-allocate (maxSize, dims) Tensors\n",
    "        self.s = np.zeros(shape=(self.maxSize, *self.stateDim), dtype=np.uint8)\n",
    "        self.a = np.zeros(self.maxSize, dtype=np.uint8)\n",
    "        self.r = np.zeros(self.maxSize, dtype=np.float32)\n",
    "        self.t = np.zeros(self.maxSize, dtype=np.uint8)\n",
    "\n",
    "        # Tables for storing the last `histLen` states. They are used for constructing the most recent agent state more easily\n",
    "        self.recent_s = []\n",
    "        self.recent_a = []\n",
    "        self.recent_t = []\n",
    "\n",
    "        # DONE pre-allocate Tensors\n",
    "        s_size = (self.histLen, *self.stateDim)\n",
    "        # use 'channels_first' because it is easier to construct array without having to reshape\n",
    "        self.buf_a = np.zeros(self.bufferSize, dtype=np.uint8)\n",
    "        self.buf_r = np.zeros(self.bufferSize, dtype=np.float32)\n",
    "        self.buf_term = np.zeros(self.bufferSize, dtype=np.uint8)\n",
    "        # shape = (bufferSize, histLen, height, width)\n",
    "        # default = (1024, 4, 105, 80)\n",
    "        self.buf_s = np.zeros(shape=(self.bufferSize, *s_size), dtype=np.uint8)\n",
    "        self.buf_s2 = np.zeros(shape=(self.bufferSize, *s_size), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):  # DONE\n",
    "        self.numEntries = 0\n",
    "        self.insertIndex = 0\n",
    "\n",
    "    def size(self):  # DONE\n",
    "        return self.numEntries\n",
    "\n",
    "    def empty(self):  # DONE\n",
    "        return self.numEntries == 0\n",
    "\n",
    "    def fill_buffer(self):  # DONE 3\n",
    "        assert self.numEntries >= self.bufferSize\n",
    "        # clear CPU buffers\n",
    "        self.buf_ind = 1\n",
    "\n",
    "        for buf_ind in range(self.bufferSize):\n",
    "            s, a, r, s2, term = self.sample_one()\n",
    "            # s.shape = (4, 105, 80)\n",
    "            # s2.shape = (4, 105, 80)\n",
    "            self.buf_s[buf_ind] = s\n",
    "            self.buf_a[buf_ind] = a\n",
    "            self.buf_r[buf_ind] = r\n",
    "            self.buf_s2[buf_ind] = s2\n",
    "            self.buf_term[buf_ind] = term\n",
    "\n",
    "    def sample_one(self):  # TODO 3\n",
    "        assert self.numEntries > 1\n",
    "\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            # start at the second index because of previous action\n",
    "            index = random.randrange(1, self.numEntries - self.recentMemSize)\n",
    "\n",
    "            # TODO 3 why do we need to check `index + self.recentMemSize - 1` instead of `index`\n",
    "            if self.t[index + self.recentMemSize - 1] == 0:\n",
    "                valid = True\n",
    "\n",
    "        return self.get(index)\n",
    "\n",
    "    def sample(self, batch_size=1):  # DONE 4\n",
    "        assert batch_size < self.bufferSize\n",
    "\n",
    "        if (self.buf_ind is None) or (self.buf_ind + batch_size) > self.bufferSize:\n",
    "            self.fill_buffer()\n",
    "\n",
    "        index = self.buf_ind\n",
    "        self.buf_ind = self.buf_ind + batch_size\n",
    "        start = index\n",
    "        end = index + batch_size\n",
    "\n",
    "        # DONE 3 only return a copy\n",
    "        s = np.copy(self.buf_s[start:end])\n",
    "        a = np.copy(self.buf_a[start:end])\n",
    "        r = np.copy(self.buf_r[start:end])\n",
    "        term = np.copy(self.buf_term[start:end])\n",
    "        s2 = np.copy(self.buf_s2[start:end])\n",
    "\n",
    "        return s, a, r, s2, term\n",
    "\n",
    "    def concatFrames(self, index, use_recent=False):  # DONE 4\n",
    "        \"\"\"\n",
    "        The `index` must not be the terminal state.\n",
    "        \"\"\"\n",
    "        if use_recent:\n",
    "            s, t = self.recent_s, self.recent_t\n",
    "        else:\n",
    "            s, t = self.s, self.t\n",
    "\n",
    "        # DONE copy frames and zeros pad missing frames\n",
    "        fullstate = np.zeros(shape=(self.histLen, *self.stateDim), dtype=np.uint8)\n",
    "\n",
    "        end_index = min(len(s) - 1, index + self.histLen)\n",
    "\n",
    "        for fs_idx, i in enumerate(range(index, end_index)):\n",
    "            fullstate[fs_idx] = np.copy(s[i])\n",
    "\n",
    "        # DONE 5 copy frames and zero-out un-related frames\n",
    "        # Because all the episode frames is stack together, \n",
    "        # the below code is use to find the terminal state index (episode-seperator) \n",
    "        # and zero out all the frames after that index.\n",
    "        zero_out = False\n",
    "\n",
    "        # start at the second frame\n",
    "        for i in range(1, self.histLen):\n",
    "            if not zero_out:\n",
    "                idx = index + i\n",
    "                # check terminal state\n",
    "                if t[idx] == 1:\n",
    "                    zero_out = True\n",
    "\n",
    "            # after terminal state is comfirmed, \n",
    "            # zero out frames starting at the terminal index\n",
    "            if zero_out:\n",
    "                fullstate[i] = np.zeros_like(fullstate[i])\n",
    "\n",
    "        return fullstate\n",
    "\n",
    "    def concatActions(self, index, use_recent=False):  # TODO 9\n",
    "        pass\n",
    "\n",
    "    def get_recent(self):  # DONE\n",
    "        # Assumes that the most recent state has been added, but the action has not\n",
    "        return self.concatFrames(0, True)\n",
    "\n",
    "    def get(self, index):  # DONE\n",
    "        s = self.concatFrames(index)\n",
    "        s2 = self.concatFrames(index + 1)\n",
    "        # TODO 3 what is `ar_index`\n",
    "        # why `ar_indxt = index + self.recentMemSize - 1`\n",
    "        ar_index = index + self.recentMemSize - 1\n",
    "\n",
    "        return s, self.a[ar_index], self.r[ar_index], s2, self.t[ar_index + 1]\n",
    "\n",
    "    def add(self, s, a, r, term):  # DONE\n",
    "        # Increment until at full capacity\n",
    "        if self.numEntries < self.maxSize:\n",
    "            self.numEntries += 1\n",
    "\n",
    "        # Always insert at next index, then wrap around\n",
    "        self.insertIndex += 1\n",
    "        # Overwrite oldest experience once at capacity\n",
    "        if self.insertIndex >= self.maxSize:\n",
    "            self.insertIndex = 0\n",
    "\n",
    "        # Overwrite (s, a, r, t) at `insertIndex`\n",
    "        self.s[self.insertIndex] = s\n",
    "        self.a[self.insertIndex] = a\n",
    "        self.r[self.insertIndex] = r\n",
    "        if term:\n",
    "            self.t[self.insertIndex] = 1\n",
    "        else:\n",
    "            self.t[self.insertIndex] = 0\n",
    "\n",
    "    def add_recent_state(self, s, term):  # DONE\n",
    "        if len(self.recent_s) == 0:\n",
    "            for i in range(self.recentMemSize):\n",
    "                self.recent_s.append(np.zeros_like(s))\n",
    "                self.recent_t.append(0)\n",
    "\n",
    "        self.recent_s.append(s)\n",
    "        if term:\n",
    "            self.recent_t.append(1)\n",
    "        else:\n",
    "            self.recent_t.append(0)\n",
    "\n",
    "        # keep recentMemSize states\n",
    "        if len(self.recent_t) > self.recentMemSize:\n",
    "            self.recent_s.pop(0)\n",
    "            self.recent_t.pop(0)\n",
    "\n",
    "    def add_recent_action(self, a):  # DONE\n",
    "        if len(self.recent_a) == 0:\n",
    "            for i in range(self.recentMemSize):\n",
    "                self.recent_a.append(0)\n",
    "\n",
    "        self.recent_a.append(a)\n",
    "\n",
    "        # keep recentMemSize steps\n",
    "        if len(self.recent_a) > self.recentMemSize:\n",
    "            self.recent_a.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions=4,\n",
    "        ep_start=1.0,\n",
    "        ep_end=0.1,\n",
    "        ep_endt=1_000_000,\n",
    "        lr=0.00025,\n",
    "        minibatch_size=1,\n",
    "        valid_size=512,\n",
    "        discount=0.99,\n",
    "        update_freq=1,\n",
    "        n_replay=1,\n",
    "        learn_start=0,\n",
    "        replay_memory=1_000_000,\n",
    "        hist_len=1,\n",
    "        max_reward=None,\n",
    "        min_reward=None,\n",
    "        network=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_actions : int\n",
    "            The number of actions that the agent can take.\n",
    "\n",
    "        ep_start : float\n",
    "            The inital epsilon value in epsilon-greedy.\n",
    "\n",
    "        ep_end : float\n",
    "            The final epsilon value in epsilon-greedy.\n",
    "\n",
    "        ep_endt : int\n",
    "            The number of timesteps over which the inital value of epislon is linearly annealed to its final value.\n",
    "\n",
    "        lr : float\n",
    "            The learning rate used by RMSProp.\n",
    "        \"\"\"\n",
    "        # self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # epsilon annealing\n",
    "        self.ep_start = ep_start  # inital epsilon value\n",
    "        self.ep = self.ep_start  # exploration probability\n",
    "        self.ep_end = ep_end  # final epsilon value\n",
    "        self.ep_endt = ep_endt  # the number of timesteps over which the inital value of epislon is linearly annealed to its final value\n",
    "\n",
    "        self.lr = lr\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.valid_size = valid_size\n",
    "\n",
    "        # Q-learning paramters\n",
    "        self.discount = discount  # discount factor\n",
    "        self.update_freq = update_freq\n",
    "        # number of points to replay per learning step\n",
    "        self.n_replay = n_replay\n",
    "        # number of steps after which learning starts\n",
    "        self.learn_start = learn_start\n",
    "        # size of the transition table\n",
    "        self.replay_memory = replay_memory\n",
    "        self.hist_len = hist_len\n",
    "        self.max_reward = max_reward\n",
    "        self.min_reward = min_reward\n",
    "\n",
    "        self.network = network if network else self.createNetwork(n_actions=n_actions)\n",
    "        self.compile_model(self.network, self.lr)\n",
    "\n",
    "        # create transition table\n",
    "        self.transitions = TransitionTable(histLen=self.hist_len, maxSize=self.replay_memory)\n",
    "\n",
    "        self.numSteps = 0  # number of perceived states\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastTerminal = None\n",
    "\n",
    "        self.valid_s = None\n",
    "        self.valid_a = None\n",
    "        self.valid_r = None\n",
    "        self.valid_s2 = None\n",
    "        self.valid_term = None\n",
    "\n",
    "    def compile_model(self, model, lr=0.00025):\n",
    "        optimizer = RMSprop(lr=lr)\n",
    "        model.compile(\n",
    "            loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy', 'mse'],\n",
    "        )\n",
    "\n",
    "    def reset(self, state):\n",
    "        # TODO 9 Low-priority\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, rawstate):  # DONE\n",
    "        state = np.mean(rawstate, axis=2, dtype=np.uint8)\n",
    "        state = state[::2, ::2]\n",
    "        # turn grayscale image to binary image\n",
    "        # _img = np.where(_img == 0, 0, 255).astype(np.uint8)\n",
    "        return state\n",
    "\n",
    "    def getQUpdate(self, s, a, r, s2, term):  # DOME 2\n",
    "        # merge `s` and `s2` together for one forward pass\n",
    "\n",
    "        term = (term * -1) + 1\n",
    "        # `s` and `s2` have to have the same shape\n",
    "        assert s.shape == s2.shape\n",
    "        forward_batch = np.concatenate((s, s2), axis=0)\n",
    "\n",
    "        # I only scale values between [0..1] at the last step to reduce memory usage\n",
    "        q_batch = self.network.predict(forward_batch / 255.0)\n",
    "        mid_point = s.shape[0]\n",
    "\n",
    "        # compute max_a Q(s_2, a)\n",
    "        q2_max = np.max(q_batch[mid_point:], axis=1)\n",
    "\n",
    "        # compute q2 = (1-terminal) * gamma * max_a Q(s2,a)\n",
    "        # this will zero out all the terminal state reward\n",
    "        q2 = q2_max * self.discount  # Matrix * number\n",
    "        q2 = q2 * term  # Matrix * Matrix\n",
    "\n",
    "        # Q(s,a) = Q(s,a) + delta\n",
    "        delta = q2 + r  # Matrix + Matrix\n",
    "\n",
    "        # (Lua) local q_all = self.network:foward(s):float()\n",
    "        q_all = q_batch[:mid_point]\n",
    "        # (Lua) q = torch.FloatTensor(q_all:size(1))\n",
    "        q = np.zeros_like(q_all.shape[0])\n",
    "        for i in range(len(q_all.shape[0])):\n",
    "            q[i] = q_all[i][a[i]]\n",
    "\n",
    "        # NeuralQLearner.lua:222\n",
    "        # (Lua) delta:add(-1, q)\n",
    "        \"\"\"\n",
    "        Lua Torch documentations (maths.md)\n",
    "\n",
    "        ```\n",
    "        > x = torch.Tensor(2, 2):fill(2)\n",
    "        > y = torch.Tensor(4):fill(3)\n",
    "        > x:add(2, y)\n",
    "        > x\n",
    "        8  8\n",
    "        8  8\n",
    "        [torch.DoubleTensor of size 2x2]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        delta = delta - q\n",
    "        delta\n",
    "        targets = q_all\n",
    "        for i in range(min(self.minibatch_size, len(a))):\n",
    "            targets[i][a[i]] = target[i]\n",
    "\n",
    "        return targets, delta, q2_max\n",
    "\n",
    "    def qLearnMinibatch(self, verbose=0):\n",
    "        # TODO accumulate losses instead of update rightaway\n",
    "        # Perform a minibatch Q-learning update:\n",
    "        # w += alpha * (r + gamma max Q(s2,a2) - Q(s,a)) * dQ(s,a)/dw\n",
    "        assert self.transitions.size() > self.minibatch_size\n",
    "\n",
    "        s, a, r, s2, term = self.transitions.sample(self.minibatch_size)\n",
    "\n",
    "        targets, delta, q2_max = self.getQUpdate(s, a, r, s2, term)\n",
    "\n",
    "        # DONE 2 what is `targets, q2_max`\n",
    "        # `targets = Q'(s)` with `Q'(s,a) = Q(s,a) + r + gamma * max_a Q(s2)`\n",
    "        # targets.shape = (batch_size, n_action)\n",
    "\n",
    "        # `q2_max` is `max_a Q(s2)`\n",
    "        # q2_max.shape = (batch_size)\n",
    "\n",
    "        self.network.fit(\n",
    "            x=(s / 255.0),\n",
    "            y=targets,\n",
    "            epochs=1,\n",
    "            batch_size=self.minibatch_size,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def sample_validation_data(self):  # DONE 9\n",
    "        # for validation\n",
    "        s, a, r, s2, term = self.transitions.sample(self.valid_size)\n",
    "        self.valid_s = s\n",
    "        self.valid_a = a\n",
    "        self.valid_r = r\n",
    "        self.valid_s2 = s2\n",
    "        self.valid_term = term\n",
    "\n",
    "    def compute_validation_statistics(self):  # TODO 9\n",
    "        # for validation\n",
    "        targets, delta, q2_max = self.getQUpdate(\n",
    "            s=self.valid_s,\n",
    "            a=self.valid_a,\n",
    "            r=self.valid_r,\n",
    "            s2=self.valid_s2,\n",
    "            term=self.valid_term,\n",
    "        )\n",
    "        avg_loss = delta.mean()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def perceive(self, reward, rawstate, terminal, testing=False, testing_ep=None, verbose=0):  # DONE 1\n",
    "        \"\"\"\n",
    "        reward : number\n",
    "            The received reward from environment.\n",
    "\n",
    "        rawstate : ndarray\n",
    "            The game screen.\n",
    "\n",
    "        terminal : int\n",
    "            If the game end then `terminal = 1` else `terminal = 0`.\n",
    "\n",
    "        testing_ep : number\n",
    "            Testing epsilon value for the epsilon-greedy algorithm.\n",
    "        \"\"\"\n",
    "        # preprocess state\n",
    "        state = self.preprocess(rawstate)\n",
    "\n",
    "        # clip reward\n",
    "        if self.max_reward is not None:\n",
    "            reward = min(reward, self.max_reward)\n",
    "\n",
    "        if self.min_reward is not None:\n",
    "            reward = max(reward, self.min_reward)\n",
    "\n",
    "        self.transitions.add_recent_state(state, terminal)\n",
    "\n",
    "        currentFullState = self.transitions.get_recent()\n",
    "\n",
    "        # store transition s, a, r, s'\n",
    "        if (self.lastState is not None) and not testing:\n",
    "            self.transitions.add(self.lastState, self.lastAction, reward, self.lastTerminal)\n",
    "\n",
    "        curState = self.transitions.get_recent()  # curState.shape == (4, 105, 80)\n",
    "        # convert to batch (1, 4, 105, 80)\n",
    "        curState = np.array([curState], dtype=np.uint8)\n",
    "\n",
    "        # select action\n",
    "        action = 0\n",
    "        if not terminal:\n",
    "            action = self.eGreedy(curState, testing_ep)\n",
    "\n",
    "        # do some Q-learning updates\n",
    "        if (self.numSteps > self.learn_start) and (not testing) and (self.numSteps % self.update_freq == 0):\n",
    "            for i in range(self.n_replay):\n",
    "                self.qLearnMinibatch(verbose=verbose)\n",
    "\n",
    "        if not testing:\n",
    "            self.numSteps += 1\n",
    "\n",
    "        self.lastState = state\n",
    "        self.lastAction = action\n",
    "        self.lastTerminal = terminal\n",
    "\n",
    "        return action\n",
    "\n",
    "    def eGreedy(self, state, testing_ep=None):  # DONE 3\n",
    "        \"\"\"\n",
    "        testing_ep : testing epsilon\n",
    "        \"\"\"\n",
    "        if testing_ep is None:\n",
    "            ep_range = self.ep_start - self.ep_end\n",
    "            ep_prog = 1.0 - max(0, self.numSteps - self.learn_start) / self.ep_endt\n",
    "            ep_delta = ep_range * ep_prog\n",
    "            self.ep = self.ep_end + max(0, ep_delta)\n",
    "        else:\n",
    "            self.ep = testing_ep\n",
    "\n",
    "        if random.random() < self.ep:\n",
    "            return random.randrange(0, self.n_actions)\n",
    "        else:\n",
    "            return self.greedy(state)\n",
    "\n",
    "    def greedy(self, state):  # DONE 6\n",
    "        q = self.network.predict(state / 255.0)[0]\n",
    "        max_q = q[0]\n",
    "        best_a = [0]\n",
    "\n",
    "        # evaluate all other actions (with random tie-breaking)\n",
    "        for a in range(1, self.n_actions):\n",
    "            if q[a] > max_q:\n",
    "                best_a = [a]\n",
    "                max_q = q[a]\n",
    "            elif q[a] == max_q:\n",
    "                best_a.append(a)\n",
    "        # random tie-breaking\n",
    "        r = random.randrange(0, len(best_a))\n",
    "        self.lastAction = best_a[r]\n",
    "        return best_a[r]\n",
    "\n",
    "    def createNetwork(self, input_shape=(4, 105, 80), n_actions=4):\n",
    "        model = keras.Sequential([\n",
    "            Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=8,\n",
    "                strides=4,\n",
    "                activation='relu',\n",
    "                input_shape=(*input_shape, ),\n",
    "                data_format='channels_first',\n",
    "            ),\n",
    "            Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                activation='relu',\n",
    "                data_format='channels_first',\n",
    "            ),\n",
    "            Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=1,\n",
    "                activation='relu',\n",
    "                data_format='channels_first',\n",
    "            ),\n",
    "            Flatten(),\n",
    "            Dense(\n",
    "                units=512,\n",
    "                activation='relu',\n",
    "            ),\n",
    "            Dense(\n",
    "                units=n_actions,\n",
    "                activation='linear',\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
