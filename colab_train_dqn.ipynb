{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "RYAoc1WBYLiv",
    "outputId": "3acff5d6-d196-484e-a36a-4d734a3199cc"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-beta1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EXbEBsjYjA7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "01xE9lFVYspZ",
    "outputId": "06e41c67-38f3-4375-b76d-b081562e96d2"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOt-GhwpYzcE"
   },
   "source": [
    "## TransitionTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnVhX1SbYux5"
   },
   "outputs": [],
   "source": [
    "class TransitionTable:\n",
    "    def __init__(\n",
    "        self,\n",
    "        stateDim=(105, 80),\n",
    "        histLen=1,\n",
    "        maxSize=1_000_000,\n",
    "        bufferSize=1024,\n",
    "    ):\n",
    "        self.stateDim = stateDim\n",
    "        self.histLen = histLen\n",
    "        self.maxSize = maxSize\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buf_ind = None\n",
    "\n",
    "        self.recentMemSize = self.histLen\n",
    "\n",
    "        self.numEntries = 0\n",
    "        self.insertIndex = 0\n",
    "\n",
    "        # The original implementation has multiple `histType`, we are going to use 'linear' only. Because of that, there is no `histIndices`\n",
    "\n",
    "        # DONE pre-allocate (maxSize, dims) Tensors\n",
    "        self.s = np.zeros(shape=(self.maxSize, *self.stateDim), dtype=np.uint8)\n",
    "        self.a = np.zeros(self.maxSize, dtype=np.uint8)\n",
    "        self.r = np.zeros(self.maxSize, dtype=np.float32)\n",
    "        self.t = np.zeros(self.maxSize, dtype=np.uint8)\n",
    "\n",
    "        # Tables for storing the last `histLen` states. They are used for constructing the most recent agent state more easily\n",
    "        self.recent_s = []\n",
    "        self.recent_a = []\n",
    "        self.recent_t = []\n",
    "\n",
    "        # DONE pre-allocate Tensors\n",
    "        s_size = (self.histLen, *self.stateDim)  # DONE 3 consider between 'channels_first' or 'channels_last'\n",
    "        # use 'channels_first' because it is easier to construct array without having to reshape\n",
    "        self.buf_a = np.zeros(self.bufferSize, dtype=np.uint8)\n",
    "        self.buf_r = np.zeros(self.bufferSize, dtype=np.float32)\n",
    "        self.buf_term = np.zeros(self.bufferSize, dtype=np.uint8)\n",
    "        # DONE 4 check the buffer shape before pass it to the model\n",
    "        # shape = (bufferSize, histLen, height, width)\n",
    "        # default = (1024, 4, 105, 80)\n",
    "        self.buf_s = np.zeros(shape=(self.bufferSize, *s_size), dtype=np.uint8)\n",
    "        self.buf_s2 = np.zeros(shape=(self.bufferSize, *s_size), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):  # DONE\n",
    "        self.numEntries = 0\n",
    "        self.insertIndex = 0\n",
    "\n",
    "    def size(self):  # DONE\n",
    "        return self.numEntries\n",
    "\n",
    "    def empty(self):  # DONE\n",
    "        return self.numEntries == 0\n",
    "\n",
    "    def fill_buffer(self):  # DONE 3\n",
    "        assert self.numEntries >= self.bufferSize\n",
    "        # clear CPU buffers\n",
    "        self.buf_ind = 1\n",
    "\n",
    "        for buf_ind in range(self.bufferSize):\n",
    "            s, a, r, s2, term = self.sample_one()\n",
    "            # s.shape = (4, 105, 80)\n",
    "            # s2.shape = (4, 105, 80)\n",
    "            self.buf_s[buf_ind] = s\n",
    "            self.buf_a[buf_ind] = a\n",
    "            self.buf_r[buf_ind] = r\n",
    "            self.buf_s2[buf_ind] = s2\n",
    "            self.buf_term[buf_ind] = term\n",
    "\n",
    "    def sample_one(self):  # TODO 3\n",
    "        assert self.numEntries > 1\n",
    "\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            # start at the second index because of previous action\n",
    "            index = random.randrange(1, self.numEntries - self.recentMemSize)\n",
    "\n",
    "            # TODO 3 why do we need to check `index + self.recentMemSize - 1` instead of `index`\n",
    "            if self.t[index + self.recentMemSize - 1] == 0:\n",
    "                valid = True\n",
    "\n",
    "        return self.get(index)\n",
    "\n",
    "    def sample(self, batch_size=1):  # DONE 4\n",
    "        assert batch_size < self.bufferSize\n",
    "\n",
    "        if (self.buf_ind is None) or (self.buf_ind + batch_size) > self.bufferSize:\n",
    "            self.fill_buffer()\n",
    "\n",
    "        index = self.buf_ind\n",
    "\n",
    "        self.buf_ind = self.buf_ind + batch_size\n",
    "\n",
    "        start = index\n",
    "        end = index + batch_size\n",
    "\n",
    "        # DONE 3 only return a copy\n",
    "        s = np.copy(self.buf_s[start:end])\n",
    "        a = np.copy(self.buf_a[start:end])\n",
    "        r = np.copy(self.buf_r[start:end])\n",
    "        term = np.copy(self.buf_term[start:end])\n",
    "        s2 = np.copy(self.buf_s2[start:end])\n",
    "\n",
    "        return s, a, r, s2, term\n",
    "\n",
    "    def concatFrames(self, index, use_recent=False):  # DONE 4\n",
    "        \"\"\"\n",
    "        The `index` must not be the terminal state\n",
    "        \"\"\"\n",
    "        if use_recent:\n",
    "            s, t = self.recent_s, self.recent_t\n",
    "        else:\n",
    "            s, t = self.s, self.t\n",
    "\n",
    "        # DONE copy frames and zeros pad missing frames\n",
    "        fullstate = np.zeros(shape=(self.histLen, *self.stateDim), dtype=np.uint8)\n",
    "\n",
    "        end_index = min(len(s) - 1, index + self.histLen)\n",
    "\n",
    "        for fs_idx, i in enumerate(range(index, end_index)):\n",
    "            fullstate[fs_idx] = np.copy(s[i])\n",
    "\n",
    "        # DONE 5 copy frames and zero-out un-related frames\n",
    "        # Because all the episode frames is stack together, the below code is use to find the terminal state index (episode-seperator) and zero out all the frames after that index.\n",
    "        zero_out = False\n",
    "\n",
    "        for i in range(1, self.histLen):\n",
    "            if not zero_out:\n",
    "                idx = index + i\n",
    "                if t[idx] == 1:\n",
    "                    zero_out = True\n",
    "\n",
    "            if zero_out:\n",
    "                fullstate[i] = np.zeros_like(fullstate[i])\n",
    "\n",
    "        return fullstate\n",
    "\n",
    "    def concatActions(self, index, use_recent=False):  # TODO 9\n",
    "        pass\n",
    "\n",
    "    def get_recent(self):  # DONE\n",
    "        # Assumes that the most recent state has been added, but the action has not\n",
    "        return self.concatFrames(0, True)\n",
    "\n",
    "    def get(self, index):  # DONE\n",
    "        s = self.concatFrames(index)\n",
    "        s2 = self.concatFrames(index + 1)\n",
    "        # TODO 3 what is `ar_index`\n",
    "        # why `ar_indxt = index + self.recentMemSize - 1`\n",
    "        ar_index = index + self.recentMemSize - 1\n",
    "\n",
    "        return s, self.a[ar_index], self.r[ar_index], s2, self.t[ar_index + 1]\n",
    "\n",
    "    def add(self, s, a, r, term):  # DONE\n",
    "        # Increment until at full capacity\n",
    "        if self.numEntries < self.maxSize:\n",
    "            self.numEntries += 1\n",
    "\n",
    "        # Always insert at next index, then wrap around\n",
    "        self.insertIndex += 1\n",
    "        # Overwrite oldest experience once at capacity\n",
    "        if self.insertIndex >= self.maxSize:\n",
    "            self.insertIndex = 0\n",
    "\n",
    "        # Overwrite (s, a, r, t) at `insertIndex`\n",
    "        self.s[self.insertIndex] = s\n",
    "        self.a[self.insertIndex] = a\n",
    "        self.r[self.insertIndex] = r\n",
    "        if term:\n",
    "            self.t[self.insertIndex] = 1\n",
    "        else:\n",
    "            self.t[self.insertIndex] = 0\n",
    "\n",
    "    def add_recent_state(self, s, term):  # DONE\n",
    "        if len(self.recent_s) == 0:\n",
    "            for i in range(self.recentMemSize):\n",
    "                self.recent_s.append(np.zeros_like(s))\n",
    "                self.recent_t.append(0)\n",
    "\n",
    "        self.recent_s.append(s)\n",
    "        if term:\n",
    "            self.recent_t.append(1)\n",
    "        else:\n",
    "            self.recent_t.append(0)\n",
    "\n",
    "        # keep recentMemSize states\n",
    "        if len(self.recent_t) > self.recentMemSize:\n",
    "            self.recent_s.pop(0)\n",
    "            self.recent_t.pop(0)\n",
    "\n",
    "    def add_recent_action(self, a):  # DONE\n",
    "        if len(self.recent_a) == 0:\n",
    "            for i in range(self.recentMemSize):\n",
    "                self.recent_a.append(0)\n",
    "\n",
    "        self.recent_a.append(a)\n",
    "\n",
    "        # keep recentMemSize steps\n",
    "        if len(self.recent_a) > self.recentMemSize:\n",
    "            self.recent_a.pop(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DODqPEPnY4UJ"
   },
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J9vTPMxY6oX"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions=4,\n",
    "        ep_start=1.0,\n",
    "        ep_end=0.1,\n",
    "        ep_endt=1_000_000,\n",
    "        lr=0.00025,\n",
    "        minibatch_size=1,\n",
    "        valid_size=500,\n",
    "        discount=0.99,\n",
    "        update_freq=1,\n",
    "        n_replay=1,\n",
    "        learn_start=0,\n",
    "        replay_memory=1_000_000,\n",
    "        hist_len=1,\n",
    "        max_reward=None,\n",
    "        min_reward=None,\n",
    "        network=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_actions : int\n",
    "            The number of actions that the agent can take.\n",
    "\n",
    "        ep_start : float\n",
    "            The inital epsilon value in epsilon-greedy.\n",
    "\n",
    "        ep_end : float\n",
    "            The final epsilon value in epsilon-greedy.\n",
    "\n",
    "        ep_endt : int\n",
    "            The number of timesteps over which the inital value of epislon is linearly annealed to its final value.\n",
    "\n",
    "        lr : float\n",
    "            The learning rate used by RMSProp.\n",
    "        \"\"\"\n",
    "        # self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # epsilon annealing\n",
    "        self.ep_start = ep_start  # inital epsilon value\n",
    "        self.ep = self.ep_start  # exploration probability\n",
    "        self.ep_end = ep_end  # final epsilon value\n",
    "        self.ep_endt = ep_endt  # the number of timesteps over which the inital value of epislon is linearly annealed to its final value\n",
    "\n",
    "        self.lr = lr\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.valid_size = valid_size\n",
    "\n",
    "        # Q-learning paramters\n",
    "        self.discount = discount  # discount factor\n",
    "        self.update_freq = update_freq\n",
    "        # number of points to replay per learning step\n",
    "        self.n_replay = n_replay\n",
    "        # number of steps after which learning starts\n",
    "        self.learn_start = learn_start\n",
    "        # size of the transition table\n",
    "        self.replay_memory = replay_memory\n",
    "        self.hist_len = hist_len\n",
    "        self.max_reward = max_reward\n",
    "        self.min_reward = min_reward\n",
    "\n",
    "        self.network = network if network else self.createNetwork(n_actions=n_actions)\n",
    "        self.compile_model(self.network, self.lr)\n",
    "\n",
    "        # create transition table\n",
    "        self.transitions = TransitionTable(histLen=self.hist_len, maxSize=self.replay_memory)\n",
    "\n",
    "        self.numSteps = 0  # number of perceived states\n",
    "        self.lastState = None\n",
    "        self.lastAction = None\n",
    "        self.lastTerminal = None\n",
    "\n",
    "    def compile_model(self, model, lr=0.00025):\n",
    "        optimizer = RMSprop(lr=lr)\n",
    "        model.compile(\n",
    "            loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy', 'mse'],\n",
    "        )\n",
    "\n",
    "    def reset(self, state):\n",
    "        # TODO 9 Low-priority\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, rawstate):  # DONE\n",
    "        state = np.mean(rawstate, axis=2, dtype=np.uint8)\n",
    "        state = state[::2, ::2]\n",
    "        # turn grayscale image to binary image\n",
    "        # _img = np.where(_img == 0, 0, 255).astype(np.uint8)\n",
    "        return state\n",
    "\n",
    "    def getQUpdate(self, s, a, r, s2, term):  # DOME 2\n",
    "        # The order of calls to forward is a bit odd\n",
    "        # in order to avoid unnecessary calls (we only need 2)\n",
    "\n",
    "        # delta = r + (1 - terminal) * gamma * max_a Q(s2, a) - Q(s, a)\n",
    "        term = (term * -1) + 1\n",
    "\n",
    "        target_q_net = self.network\n",
    "\n",
    "        # compute max_a Q(s_2, a)\n",
    "        q2_max = np.max(target_q_net.predict(s2 / 255.0), axis=1)\n",
    "\n",
    "        # compute q2 = (1-terminal) * gamma * max_a Q(s2,a)\n",
    "        q2 = q2_max * self.discount\n",
    "        q2 = q2 * term\n",
    "\n",
    "        delta = r + q2\n",
    "\n",
    "        q_all = self.network.predict(s / 255.0)\n",
    "        q = np.zeros(len(q_all), dtype=np.float32)\n",
    "        for i in range(len(q_all)):\n",
    "            q[i] = q_all[i][a[i]]\n",
    "\n",
    "        delta = delta + (q * -1)\n",
    "\n",
    "        targets = np.zeros(shape=(self.minibatch_size, self.n_actions), dtype=np.float32)\n",
    "        for i in range(min(self.minibatch_size, len(a))):\n",
    "            targets[i][a[i]] = delta[i]\n",
    "\n",
    "        return targets, delta, q2_max\n",
    "\n",
    "    def qLearnMinibatch(self):  # DONE 4\n",
    "        # perform a minibatch Q-learning update:\n",
    "        # w += alpha * (r + gamma max Q(s2,a2) - Q(s,a)) * dQ(s,a)/dw\n",
    "\n",
    "        # w = w + (gamma max Q(s2, a2) - Q(s,a)) # this is the label for Keras\n",
    "        assert self.transitions.size() > self.minibatch_size\n",
    "\n",
    "        s, a, r, s2, term = self.transitions.sample(self.minibatch_size)\n",
    "\n",
    "        targets, delta, q2_max = self.getQUpdate(s, a, r, s2, term)\n",
    "\n",
    "        # DONE 2 what is `targets, delta, q2_max`\n",
    "        # delta = r + (1-term) * gamma * max_a Q(s2, a) - Q(s, a)\n",
    "\n",
    "        # targets.shape = (batch_size, n_action)\n",
    "        # delta.shape = (batch_size)\n",
    "        # q2_max.shape = (batch_size)\n",
    "\n",
    "        self.network.fit(\n",
    "            x=(s / 255.0),\n",
    "            y=targets,\n",
    "            epochs=1,\n",
    "            batch_size=self.minibatch_size,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "    def sample_validation_data(self):  # TODO 9\n",
    "        # for validation\n",
    "        pass\n",
    "\n",
    "    def sample_validation_statistics(self):  # TODO 9\n",
    "        # for validation\n",
    "        pass\n",
    "\n",
    "    def perceive(self, reward, rawstate, terminal, testing=False, testing_ep=None):  # DONE 1\n",
    "        \"\"\"\n",
    "        reward : number\n",
    "            The received reward from environment.\n",
    "\n",
    "        rawstate : ndarray\n",
    "            The game screen.\n",
    "\n",
    "        terminal : int\n",
    "            If the game end then `terminal = 1` else `terminal = 0`.\n",
    "\n",
    "        testing_ep : number\n",
    "            Testing epsilon value for the epsilon-greedy algorithm.\n",
    "        \"\"\"\n",
    "        # preprocess state\n",
    "        state = self.preprocess(rawstate)\n",
    "        # clip reward\n",
    "        if self.max_reward is not None:\n",
    "            reward = min(reward, self.max_reward)\n",
    "\n",
    "        if self.min_reward is not None:\n",
    "            reward = max(reward, self.min_reward)\n",
    "\n",
    "        self.transitions.add_recent_state(state, terminal)\n",
    "\n",
    "        currentFullState = self.transitions.get_recent()\n",
    "\n",
    "        # store transition s, a, r, s'\n",
    "        if (self.lastState is not None) and not testing:\n",
    "            self.transitions.add(self.lastState, self.lastAction, reward, self.lastTerminal)\n",
    "\n",
    "        curState = self.transitions.get_recent()  # DONE (4, 105, 80)\n",
    "        curState = np.array([curState], dtype=np.uint8)\n",
    "\n",
    "        # select action\n",
    "        action = 0\n",
    "        if not terminal:\n",
    "            action = self.eGreedy(curState, testing_ep)\n",
    "\n",
    "        # do some Q-learning updates\n",
    "        if (self.numSteps > self.learn_start) and (not testing) and (self.numSteps % self.update_freq == 0):\n",
    "            for i in range(self.n_replay):\n",
    "                self.qLearnMinibatch()\n",
    "\n",
    "        if not testing:\n",
    "            self.numSteps += 1\n",
    "\n",
    "        self.lastState = state\n",
    "        self.lastAction = action\n",
    "        self.lastTerminal = terminal\n",
    "\n",
    "        return action\n",
    "\n",
    "    def eGreedy(self, state, testing_ep=None):  # DONE 3\n",
    "        if testing_ep is None:\n",
    "            self.ep = self.ep_end + max(0, (self.ep_start - self.ep_end) * (self.ep_endt - max(0, self.numSteps - self.learn_start)) / self.ep_endt)\n",
    "        else:\n",
    "            self.ep = testing_ep\n",
    "\n",
    "        if random.random() < self.ep:\n",
    "            return random.randrange(0, self.n_actions)\n",
    "        else:\n",
    "            return self.greedy(state)\n",
    "\n",
    "    def greedy(self, state):  # DONE 6\n",
    "        q = self.network.predict(state / 255.0)[0]\n",
    "        max_q = q[0]\n",
    "        best_a = [0]\n",
    "\n",
    "        # evaluate all other actions (with random tie-breaking)\n",
    "        for a in range(1, self.n_actions):\n",
    "            if q[a] > max_q:\n",
    "                best_a = [a]\n",
    "                max_q = q[a]\n",
    "            elif q[a] == max_q:\n",
    "                best_a.append(a)\n",
    "\n",
    "        r = random.randrange(0, len(best_a))\n",
    "\n",
    "        self.lastAction = best_a[r]\n",
    "\n",
    "        return best_a[r]\n",
    "\n",
    "    def createNetwork(self, input_shape=(4, 105, 80), n_actions=4):\n",
    "        model = keras.Sequential([\n",
    "            Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=8,\n",
    "                strides=4,\n",
    "                activation='relu',\n",
    "                input_shape=(*input_shape, ),\n",
    "                data_format='channels_first',\n",
    "            ),\n",
    "            Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=4,\n",
    "                strides=2,\n",
    "                activation='relu',\n",
    "            ),\n",
    "            Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=1,\n",
    "                activation='relu',\n",
    "            ),\n",
    "            Flatten(),\n",
    "            Dense(\n",
    "                units=512,\n",
    "                activation='relu',\n",
    "            ),\n",
    "            Dense(\n",
    "                units=n_actions,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EHEeNlhZBRI"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoKB2CSpZEtW"
   },
   "outputs": [],
   "source": [
    "# hyperparamters\n",
    "minibatch_size = 32  # number of training cases over which each stochastic gradient descent (SGD) update is computed\n",
    "replay_memory_size = 500_000  # SGD updates are sampled from this number of most recent frames\n",
    "agent_history_length = 4  # the number of most recent frames experienced by agent that are given as input to the Q network\n",
    "target_network_update_frequency = 10_000  # the freuquency (measured in the number of parameter updates) with which the target netwrok is updated (this corresponds to the parameter C from Algorithm 1)\n",
    "discount_factor = 0.99  # discount factor gamma used in the Q-learning update\n",
    "action_repeat = 4  # repeat each action selected by the agent this many times. Using a value of 4 results in the agent seeing only every 4 input frame\n",
    "update_frequency = 4  # the number of actions selected by the agent between successive SGD updates. Using a value of 4 results in the agent selecting 4 actions between each pair of successive updates\n",
    "learning_rate = 0.00025  # the learning rate used by RMSProp\n",
    "gradient_momentum = 0.95  # squared gradient (denominator) momentum used by RMSProp\n",
    "min_squared_gradient = 0.01  # constant added to the squared gradient in the denominator of the RMSProp update\n",
    "inital_exploration = 1.0  # initial value of epsilon in epsilon-greedy exploration\n",
    "final_exploration = 0.1  # final value of epsilon in epsilon-greedy exploration\n",
    "final_exploration_frame = 1_000_000  # the number of frames over which the initial value of epsilon is linearly annealed to its final value\n",
    "replay_start_size = 50_000  # a uniform random policy is run for this number of frames before learning starts and the resulting experience is used to populate the replay memory\n",
    "no_op_max = 30  # maximum number of \"do nothing\" actions to be performed by agent at the start of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZ3gHHdQZW-K"
   },
   "source": [
    "## General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwF1XQv7ZQh9"
   },
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "# general setup\n",
    "env = gym.make(env_name)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(\n",
    "    n_actions=n_actions,\n",
    "    ep_start=inital_exploration,\n",
    "    ep_end=final_exploration,\n",
    "    ep_endt=final_exploration_frame,\n",
    "    lr=learning_rate,\n",
    "    minibatch_size=minibatch_size,\n",
    "    discount=discount_factor,\n",
    "    update_freq=update_frequency,\n",
    "    learn_start=replay_start_size,\n",
    "    replay_memory=replay_memory_size,\n",
    "    hist_len=agent_history_length,\n",
    "    max_reward=1,\n",
    "    min_reward=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "G3GgZ-MJZaY1",
    "outputId": "7884573b-f68b-48df-eb0f-c53179f58531"
   },
   "outputs": [],
   "source": [
    "agent.network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYqtEpazZxd8"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "You can run this cell multiple times to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "k3V7qQD0ZwQU",
    "outputId": "5f6da0f3-ed81-4cfe-b188-0a12b87c1037"
   },
   "outputs": [],
   "source": [
    "num_steps = 500_000\n",
    "step = 0\n",
    "\n",
    "screen = env.reset()\n",
    "reward = 0\n",
    "terminal = 0\n",
    "\n",
    "train_start = time.time()\n",
    "for step in tqdm(range(num_steps)):\n",
    "    action = agent.perceive(reward, screen, terminal)\n",
    "\n",
    "    # game over? get next game!\n",
    "    if not terminal:\n",
    "        observation = env.step(action)\n",
    "        # env.render()\n",
    "        screen, reward, done, info = observation\n",
    "        if done:\n",
    "            terminal = 1\n",
    "    else:\n",
    "        screen = env.reset()\n",
    "        reward = 0\n",
    "        terminal = 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_train_dqn",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
